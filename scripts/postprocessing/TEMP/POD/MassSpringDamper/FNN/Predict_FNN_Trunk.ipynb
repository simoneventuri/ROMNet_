{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Postprocessing a FNN Trained for a Mass-Spring-Damper System "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Loading Libraries\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import numpy                              as np\n",
    "import pandas                             as pd\n",
    "\n",
    "import matplotlib\n",
    "from matplotlib                           import pyplot as plt\n",
    "WORKSPACE_PATH  = os.environ['WORKSPACE_PATH']\n",
    "plt.style.use(WORKSPACE_PATH+'/ROMNet/romnet/extra/postprocessing/presentation.mplstyle')\n",
    "\n",
    "from tensorflow                           import keras\n",
    "\n",
    "import romnet                             as rmnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Input Data\n",
    "\n",
    "ROMNet_fld    = WORKSPACE_PATH + '/ROMNet/romnet/'\n",
    "#path_to_run_fld  = WORKSPACE_PATH + '/ROMNet/MSD_100Cases_POD_2_Trunk/FNN/Deterministic/Run_1/'\n",
    "path_to_run_fld  = WORKSPACE_PATH + '/ROMNet/MSD_100Cases_POD_All_Trunk/FNN/Deterministic/Run_1/'\n",
    "\n",
    "#path_to_run_fld  = WORKSPACE_PATH + '/ROMNet/MSD_100Cases_1_Trunk/FNN_BbB/Deterministic/Run_10/'\n",
    "#path_to_run_fld  = WORKSPACE_PATH + '/ROMNet/MSD_10Cases/FNN/BayesByBackprop/Run_2/'\n",
    "\n",
    "TrainingCases = [0]#[0,2,4,6,8]\n",
    "TestCases     = [0,2,4]#[0,2]\n",
    "\n",
    "NSamples      = 100\n",
    "\n",
    "Nt            = 100\n",
    "tout          = np.linspace(0.,15.,Nt)\n",
    "SOLVER        = 'Radau'\n",
    "\n",
    "LineTypeVec   = ['-',':','--','.-']*10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[ROMNet]: Reading Input File from:  /Users/sventuri/WORKSPACE//ROMNet/MSD_100Cases_POD_All_Trunk/FNN/Deterministic/Run_1/\n",
      "\n",
      "[ROMNet]: Keep Loading Modules and Functions...\n",
      "\n",
      "[ROMNet]: Initializing Input ...\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n[ROMNet]: Reading Input File from: \", path_to_run_fld)\n",
    "sys.path.insert(0, path_to_run_fld)\n",
    "\n",
    "print(\"\\n[ROMNet]: Keep Loading Modules and Functions...\")\n",
    "from ROMNet_Input import inputdata\n",
    "\n",
    "print(\"\\n[ROMNet]: Initializing Input ...\")\n",
    "InputData               = inputdata(WORKSPACE_PATH, ROMNet_fld)\n",
    "\n",
    "\n",
    "InputData.InputFilePath = path_to_run_fld+'/ROMNet_Input.py'\n",
    "InputData.train_int_flg   = 0\n",
    "InputData.path_to_run_fld  = path_to_run_fld"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[ROMNet - model_deterministic.py    ]:   Initializing the ML Model\n",
      "\n",
      "[ROMNet - model_deterministic.py    ]:   Building the ML Model\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'inputdata' object has no attribute 'input_vars_all'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-86eba510973f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrmnt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModel_Deterministic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mInputData\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mInputData\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNet\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mNN\u001b[0m    \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/ROMNet-0.0.1-py3.8.egg/romnet/utils/internal.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtimeit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefault_timer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0mte\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtimeit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefault_timer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"%r took %f s\\n\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mte\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/ROMNet-0.0.1-py3.8.egg/romnet/model/model_deterministic.py\u001b[0m in \u001b[0;36mbuild\u001b[0;34m(self, InputData, data, Net, loadfile_no)\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m         \u001b[0;31m#-----------------------------------------------------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 158\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mInputData\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAntiPCA_flg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/ROMNet-0.0.1-py3.8.egg/romnet/nn/fnn.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, InputData, norm_input)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_vars\u001b[0m      \u001b[0;34m=\u001b[0m \u001b[0mInputData\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_vars_all\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_inputs\u001b[0m        \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_vars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'inputdata' object has no attribute 'input_vars_all'"
     ]
    }
   ],
   "source": [
    "surrogate_type = InputData.surrogate_type\n",
    "if (surrogate_type == 'FNN-SourceTerms'):\n",
    "    surrogate_type = 'FNN'\n",
    "\n",
    "Net   = getattr(rmnt.nn, surrogate_type)\n",
    "\n",
    "model = rmnt.model.Model_Deterministic(InputData)\n",
    "\n",
    "model.build(InputData, None, Net)\n",
    "\n",
    "NN    = model.net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Evaluating on Test Data\n",
    "\n",
    "VarOI     = 'POD_1'\n",
    "\n",
    "Vars      = ['POD_'+str(i_mode+1) for i_mode in range(InputData.n_modes)]\n",
    "\n",
    "FileName  = InputData.path_to_data_fld + '/test/ext/Input.csv'# + InputData.InputFile\n",
    "Data      = pd.read_csv(FileName, header=0)\n",
    "tVec      = Data['t'].to_numpy()[...,np.newaxis]\n",
    "VarNames  = list(Data.columns).remove('t')\n",
    "\n",
    "FileName  = InputData.path_to_data_fld + '/test/ext/Output.csv'# + InputData.InputFile\n",
    "Data      = pd.read_csv(FileName, header=0)\n",
    "Output    = Data[Vars].to_numpy()\n",
    "\n",
    "\n",
    "## Variable to Be Visualized\n",
    "for iVar in range(len(Vars)):\n",
    "    if (Vars[iVar] == VarOI):\n",
    "        print('iVar = ', iVar)\n",
    "        break\n",
    "\n",
    "        \n",
    "yMat = NN.predict(tVec)    \n",
    "\n",
    "plt.figure(figsize=(10,8))\n",
    "plt.plot(tVec, Output[:,iVar],  'k', label='From SVD')\n",
    "plt.plot(tVec, yMat[:,iVar],    'r', label='From FNN')\n",
    "plt.xlabel('t')\n",
    "plt.ylabel(r'$\\Psi_{x_'+str(iVar+1)+'}$')\n",
    "#plt.xscale('log')\n",
    "#plt.xlim([1.e-6,1.e-2])\n",
    "plt.legend()\n",
    "#plt.show()\n",
    "\n",
    "\n",
    "tVecTtry = np.linspace(0., 30., 3000)[...,np.newaxis]\n",
    "yMatTry  = NN.predict(tVecTtry)    \n",
    "\n",
    "plt.figure(figsize=(10,8))\n",
    "plt.plot(tVec, Output[:,iVar],      'k', label='From SVD')\n",
    "plt.plot(tVecTtry, yMatTry[:,iVar], 'b', label='From FNN')\n",
    "plt.xlabel('t')\n",
    "plt.ylabel(r'$\\Psi_{x_'+str(iVar+1)+'}$')\n",
    "#plt.xscale('log')\n",
    "#plt.xlim([1.e-6,1.e-2])\n",
    "plt.legend()\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('[PCA]  Max % Error = ', np.max(abs((yMat - Output)/yMat)*100))\n",
    "print('[PCA]  Max      SE = ', np.max((yMat - Output)**2))\n",
    "print('[PCA] Mean % Error = ', np.mean(abs((yMat - Output)/yMat)*100))\n",
    "print('[PCA]          MSE = ', np.mean((yMat - Output)**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NN.layers[0].weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data = pd.read_csv(path_to_run_fld+'/Training/History.csv')\n",
    "\n",
    "fig  = plt.figure(figsize=(12,8))\n",
    "plt.plot(Data['tot_loss'],     label='Training')\n",
    "plt.plot(Data['val_tot_loss'], label='Validation')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Tot. Loss [MSE]')\n",
    "plt.legend()\n",
    "plt.yscale('log')\n",
    "\n",
    "\n",
    "fig  = plt.figure(figsize=(12,8))\n",
    "plt.plot(Data['pts_loss'],     label='Training')\n",
    "plt.plot(Data['val_pts_loss'], label='Validation')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Data Loss [MSE]')\n",
    "plt.legend()\n",
    "plt.yscale('log')\n",
    "#plt.ylim([1.e-2, 1.e0])\n",
    "# fig  = plt.figure(figsize=(12,8))\n",
    "# plt.plot(Data['scs_loss'],     label='Training')\n",
    "# plt.plot(Data['val_scs_loss'], label='Validation')\n",
    "# plt.xlabel('Epoch')\n",
    "# plt.ylabel('S.C.s Loss [MSE]')\n",
    "# plt.legend()\n",
    "# plt.yscale('log')\n",
    "\n",
    "# fig  = plt.figure(figsize=(12,8))\n",
    "# plt.plot(Data['ics_loss'],     label='Training')\n",
    "# plt.plot(Data['val_ics_loss'], label='Validation')\n",
    "# plt.xlabel('Epoch')\n",
    "# plt.ylabel('I.C.s Loss [MSE]')\n",
    "# plt.legend()\n",
    "# plt.yscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OutputDir = InputData.path_to_data_fld\n",
    "\n",
    "FileName = OutputDir+'/../ROM/A.csv'\n",
    "A        = pd.read_csv(FileName, delimiter=',', header=None).to_numpy()\n",
    "\n",
    "FileName = OutputDir+'/../ROM/C.csv'\n",
    "C        = pd.read_csv(FileName, delimiter=',', header=None).to_numpy()\n",
    "\n",
    "FileName = OutputDir+'/../ROM/D.csv'\n",
    "D        = pd.read_csv(FileName, delimiter=',', header=None).to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yMat_pod_  = NN.predict(tVec)    \n",
    "yMat_      = (yMat_pod_.dot(A))*D[:,0] + C[:,0]\n",
    "\n",
    "\n",
    "plt.figure(figsize=(12,10))\n",
    "plt.plot(tVec, yMat_[:,50],  'k')\n",
    "plt.xlabel('t')\n",
    "#plt.xscale('log')\n",
    "#plt.legend()\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import tensorflow as tf\n",
    "from keras    import backend\n",
    "from keras    import __version__ as keras_version  # pylint: disable=g-import-not-at-top\n",
    "from keras.saving import saving_utils\n",
    "\n",
    "\n",
    "def _legacy_weights(layer):\n",
    "    \"\"\"DO NOT USE.\n",
    "    For legacy reason, the layer.weights was in the order of\n",
    "    [self.trainable_weights + self.non_trainable_weights], and this order was\n",
    "    used for preserving the weights in h5 format. The new order of layer.weights\n",
    "    are the same as layer.get_weights() which is more intuitive for user. To\n",
    "    keep supporting the existing saved h5 file, this method should be used to\n",
    "    save/load weights. In future version, we will delete this method and\n",
    "    introduce a breaking change for h5 and stay with the new order for weights.\n",
    "    Args:\n",
    "    layer: a `tf.keras.Model` or `tf.keras.layers.Layer` instance.\n",
    "    Returns:\n",
    "    A list of variables with the order of trainable_weights, followed by\n",
    "      non_trainable_weights.\n",
    "    \"\"\"\n",
    "    weights = layer.trainable_weights + layer.non_trainable_weights\n",
    "    if any(not isinstance(w, tf.Variable) for w in weights):\n",
    "        raise NotImplementedError(\n",
    "            f'Save or restore weights that is not an instance of `tf.Variable` is '\n",
    "            f'not supported in h5, use `save_format=\\'tf\\'` instead. Received a '\n",
    "            f'model or layer {layer.__class__.__name__} with weights {weights}')\n",
    "    return weights\n",
    "\n",
    "\n",
    "\n",
    "def save_attributes_to_hdf5_group(group, name, data):\n",
    "    HDF5_OBJECT_HEADER_LIMIT = 64512\n",
    "    \n",
    "    bad_attributes = [x for x in data if len(x) > HDF5_OBJECT_HEADER_LIMIT]\n",
    "\n",
    "    # Expecting this to never be true.\n",
    "    if bad_attributes:\n",
    "        raise RuntimeError('The following attributes cannot be saved to HDF5 '\n",
    "                           'file because they are larger than %d bytes: %s' %\n",
    "                           (HDF5_OBJECT_HEADER_LIMIT, ', '.join(bad_attributes)))\n",
    "\n",
    "    data_npy = np.asarray(data)\n",
    "\n",
    "    num_chunks = 1\n",
    "    chunked_data = np.array_split(data_npy, num_chunks)\n",
    "\n",
    "    # This will never loop forever thanks to the test above.\n",
    "    while any(x.nbytes > HDF5_OBJECT_HEADER_LIMIT for x in chunked_data):\n",
    "        num_chunks += 1\n",
    "        chunked_data = np.array_split(data_npy, num_chunks)\n",
    "\n",
    "    if num_chunks > 1:\n",
    "        for chunk_id, chunk_data in enumerate(chunked_data):\n",
    "            group.attrs['%s%d' % (name, chunk_id)] = chunk_data\n",
    "    else:\n",
    "        group.attrs[name] = data\n",
    "\n",
    "        \n",
    "def load_attributes_from_hdf5_group(group, name):\n",
    "    \"\"\"Loads attributes of the specified name from the HDF5 group.\n",
    "    This method deals with an inherent problem\n",
    "    of HDF5 file which is not able to store\n",
    "    data larger than HDF5_OBJECT_HEADER_LIMIT bytes.\n",
    "    Args:\n",
    "      group: A pointer to a HDF5 group.\n",
    "      name: A name of the attributes to load.\n",
    "    Returns:\n",
    "      data: Attributes data.\n",
    "    \"\"\"\n",
    "    if name in group.attrs:\n",
    "        data = [\n",
    "            n.decode('utf8') if hasattr(n, 'decode') else n\n",
    "            for n in group.attrs[name]\n",
    "        ]\n",
    "    else:\n",
    "        data = []\n",
    "        chunk_id = 0\n",
    "        while '%s%d' % (name, chunk_id) in group.attrs:\n",
    "            data.extend([\n",
    "              n.decode('utf8') if hasattr(n, 'decode') else n\n",
    "              for n in group.attrs['%s%d' % (name, chunk_id)]\n",
    "          ])\n",
    "        chunk_id += 1\n",
    "    return data\n",
    "\n",
    "\n",
    "def save_weights_to_hdf5_group(f, layers, old_string, new_string):\n",
    "\n",
    "    layer_names_temp = load_attributes_from_hdf5_group(f_new, 'layer_names')\n",
    "    layer_names_old  = []\n",
    "    for layer in layer_names_temp:\n",
    "        layer_names_old.append(layer.encode('utf8'))\n",
    "    \n",
    "    layer_names = []\n",
    "    for layer in layers:\n",
    "        if (old_string in layer.name):\n",
    "            layer_name = layer.name.replace(old_string, new_string) #############\n",
    "            layer_names.append(layer_name.encode('utf8'))\n",
    "            layer._name = layer_name\n",
    "\n",
    "    save_attributes_to_hdf5_group(f, 'layer_names', layer_names+layer_names_old)\n",
    "    f.attrs['backend'] = backend.backend().encode('utf8')\n",
    "    f.attrs['keras_version'] = str(keras_version).encode('utf8')\n",
    "\n",
    "    # Sort model layers by layer name to ensure that group names are strictly\n",
    "    # growing to avoid prefix issues.\n",
    "    for layer in sorted(layers, key=lambda x: x.name):\n",
    "        if (new_string in layer.name):\n",
    "            layer_name    = layer.name.replace(old_string, new_string) ############# \n",
    "            g             = f.create_group(layer_name)\n",
    "            weights       = _legacy_weights(layer)\n",
    "            weight_values = backend.batch_get_value(weights)\n",
    "\n",
    "            weight_names = []\n",
    "            for w in weights:\n",
    "                w_name = w.name.replace(old_string, new_string) ############# \n",
    "                weight_names.append(w_name.encode('utf8'))\n",
    "            #weight_names = [w.name.encode('utf8') for w in weights]\n",
    "            save_attributes_to_hdf5_group(g, 'weight_names', weight_names)\n",
    "            for name, val in zip(weight_names, weight_values):\n",
    "                param_dset = g.create_dataset(name, val.shape, dtype=val.dtype)\n",
    "                if not val.shape:\n",
    "                    # scalar\n",
    "                    param_dset[()] = val\n",
    "                else:\n",
    "                    param_dset[:] = val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    os.makedirs(InputData.path_to_data_fld+'/../../FNN/')\n",
    "except:\n",
    "    pass\n",
    "filename_new = InputData.path_to_data_fld+'/../../FNN/Final.h5'\n",
    "\n",
    "f_new        = h5py.File(filename_new, 'a')\n",
    "\n",
    "save_weights_to_hdf5_group(f_new, NN.layers, 'Trunk_1', 'Trunk_'+str(InputData.i_red))\n",
    "\n",
    "f_new.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f_new        = h5py.File(filename_new, 'a')\n",
    "\n",
    "# data = load_attributes_from_hdf5_group(f_new, 'layer_names')\n",
    "# data\n",
    "\n",
    "# f_new.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_new.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.keras.saving.hdf5_format import save_weights_to_hdf5_group as save_weights_to_hdf5_group_\n",
    "\n",
    "# try:\n",
    "#     os.makedirs(InputData.path_to_data_fld+'/../../FNN/')\n",
    "# except:\n",
    "#     pass\n",
    "# filename_new = InputData.path_to_data_fld+'/../../FNN/Final_.h5'\n",
    "\n",
    "# f_new        = h5py.File(filename_new, 'w')\n",
    "\n",
    "# save_weights_to_hdf5_group_(f_new, NN.layers)\n",
    "\n",
    "# f_new.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f_new.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
