{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Postprocessing a FNN Trained for a Mass-Spring-Damper System "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Loading Libraries\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import numpy                              as np\n",
    "import pandas                             as pd\n",
    "\n",
    "import matplotlib\n",
    "from matplotlib                           import pyplot as plt\n",
    "#WORKSPACE_PATH = os.environ['WORKSPACE_PATH']\n",
    "WORKSPACE_PATH = os.getcwd()+'/../../../../../../../'\n",
    "plt.style.use(WORKSPACE_PATH+'/ROMNet/romnet/extra/postprocessing/presentation.mplstyle')\n",
    "\n",
    "from tensorflow                           import keras\n",
    "\n",
    "import romnet                             as rmnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Input Data\n",
    "\n",
    "i_red          = 1\n",
    "\n",
    "ROMNet_fld    = WORKSPACE_PATH + '/ROMNet/romnet/'\n",
    "\n",
    "#path_to_run_fld  = WORKSPACE_PATH + '/ROMNet/MSD_100Cases_POD_'+str(i_red)+'_Branch/FNN/Run_1/'\n",
    "path_to_run_fld  = WORKSPACE_PATH + '/ROMNet/MSD_100Cases_POD_All_Branch_'+str(i_red)+'/FNN/Run_1/'\n",
    "\n",
    "#path_to_run_fld  = WORKSPACE_PATH + '/ROMNet/MSD_100Cases_POD_All_Branch_'+str(i_red)+'/FNN_BbB/Run_1/'\n",
    "\n",
    "TrainingCases = [0]#[0,2,4,6,8]\n",
    "TestCases     = [0,2,4]#[0,2]\n",
    "\n",
    "NSamples      = 100\n",
    "\n",
    "Nt            = 100\n",
    "tout          = np.linspace(0.,15.,Nt)\n",
    "SOLVER        = 'Radau'\n",
    "\n",
    "LineTypeVec   = ['-',':','--','.-']*10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[ROMNet]: Reading Input File from:  /Users/sventuri/WORKSPACE/ROMNet/romnet/scripts/postprocessing/POD/MassSpringDamper/FNN/../../../../../../..//ROMNet/MSD_100Cases_POD_All_Branch_1/FNN/Run_1/\n",
      "\n",
      "[ROMNet]: Keep Loading Modules and Functions...\n",
      "\n",
      "[ROMNet]: Initializing Input ...\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n[ROMNet]: Reading Input File from: \", path_to_run_fld)\n",
    "sys.path.insert(0, path_to_run_fld)\n",
    "\n",
    "print(\"\\n[ROMNet]: Keep Loading Modules and Functions...\")\n",
    "from ROMNet_Input import inputdata\n",
    "\n",
    "print(\"\\n[ROMNet]: Initializing Input ...\")\n",
    "InputData               = inputdata(WORKSPACE_PATH)\n",
    "\n",
    "\n",
    "InputData.InputFilePath = path_to_run_fld+'/ROMNet_Input.py'\n",
    "InputData.train_int_flg = 0\n",
    "InputData.path_to_run_fld  = path_to_run_fld"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[ROMNet - model_tf.py    ]:   Initializing the ML Model\n",
      "\n",
      "[ROMNet - model_tf.py    ]:   Building the ML Model\n",
      "\n",
      "[ROMNet - fnn.py                    ]:   Constructing Feed-Forward Network: \n",
      "[ROMNet - system_of_components.py   ]:     Constructing System of Components: FNN\n",
      "[ROMNet - component.py              ]:       Constructing Component: FNN\n",
      "[ROMNet - sub_component.py          ]:         Constructed Sub-Component: Main with Layers:       ListWrapper([<romnet.nn.building_blocks.normalization.CustomNormalization object at 0x174087b50>, <keras.layers.core.dense.Dense object at 0x174463970>, <keras.layers.core.dropout.Dropout object at 0x1744cdc40>, <keras.layers.core.dense.Dense object at 0x1744873a0>, <keras.layers.core.dropout.Dropout object at 0x17449c6d0>, <keras.layers.core.dense.Dense object at 0x17449c8b0>, <keras.layers.core.dropout.Dropout object at 0x17449c640>, <keras.layers.core.dense.Dense object at 0x17449c9d0>])\n",
      "\n",
      "[ROMNet - model_tf.py    ]:   Loading ML Model Parameters from File:  /Users/sventuri/WORKSPACE/ROMNet/romnet/scripts/postprocessing/POD/MassSpringDamper/FNN/../../../../../../..//ROMNet/MSD_100Cases_POD_All_Branch_1/FNN/Run_1//Training/Params//000438.h5\n",
      "[ROMNet - hdf5_format.py            ]:     Loaded Weights for Layer  FNN-FNN_Normalization\n",
      "[ROMNet - hdf5_format.py            ]:     Loaded Weights for Layer  FNN-FNN_Normalization\n",
      "[ROMNet - hdf5_format.py            ]:     Loaded Weights for Layer  FNN-FNN_Normalization\n",
      "[ROMNet - hdf5_format.py            ]:     Loaded Weights for Layer  FNN-FNN_Normalization\n",
      "[ROMNet - hdf5_format.py            ]:     Loaded Weights for Layer  FNN-FNN_Normalization\n",
      "[ROMNet - hdf5_format.py            ]:     Loaded Weights for Layer  FNN-FNN-Main-HL_1\n",
      "[ROMNet - hdf5_format.py            ]:     Loaded Weights for Layer  FNN-FNN-Main-HL_1\n",
      "[ROMNet - hdf5_format.py            ]:     Loaded Weights for Layer  FNN-FNN-Main-HL_2\n",
      "[ROMNet - hdf5_format.py            ]:     Loaded Weights for Layer  FNN-FNN-Main-HL_2\n",
      "[ROMNet - hdf5_format.py            ]:     Loaded Weights for Layer  FNN-FNN-Main-HL_3\n",
      "[ROMNet - hdf5_format.py            ]:     Loaded Weights for Layer  FNN-FNN-Main-HL_3\n",
      "[ROMNet - hdf5_format.py            ]:     Loaded Weights for Layer  FNN-FNN-Main-HL_4\n",
      "[ROMNet - hdf5_format.py            ]:     Loaded Weights for Layer  FNN-FNN-Main-HL_4\n",
      "'load_params' took 0.022680 s\n",
      "\n",
      "\n",
      "[ROMNet - model_tf.py    ]:   Saving ML Model Parameters to File:  /Users/sventuri/WORKSPACE/ROMNet/romnet/scripts/postprocessing/POD/MassSpringDamper/FNN/../../../../../../..//ROMNet/MSD_100Cases_POD_All_Branch_1/FNN/Run_1//Model/Params/Initial.h5\n",
      "'save_params' took 0.021783 s\n",
      "\n",
      "'build' took 1.476319 s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "surrogate_type = InputData.surrogate_type\n",
    "if (surrogate_type == 'FNN-SourceTerms'):\n",
    "    surrogate_type = 'FNN'\n",
    "\n",
    "Net   = getattr(rmnt.nn, surrogate_type)\n",
    "\n",
    "model = rmnt.model.Model_TF(InputData)\n",
    "\n",
    "if (InputData.phys_system is not None):\n",
    "    System = getattr(rmnt.pinn.system, InputData.phys_system)\n",
    "    system = System(InputData)\n",
    "    \n",
    "model.build(InputData, None, Net, system)#, loadfile_no='000027')\n",
    "\n",
    "NN    = model.net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'a' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-64e8d0d88e83>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m### Evaluating on Test Data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0ma\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0miVar\u001b[0m       \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mVarOI\u001b[0m     \u001b[0;34m=\u001b[0m \u001b[0;34m'POD_2'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'a' is not defined"
     ]
    }
   ],
   "source": [
    "### Evaluating on Test Data\n",
    "\n",
    "iVar       = 1\n",
    "\n",
    "VarOI     = 'POD_2'\n",
    "\n",
    "Vars      = ['POD_'+str(i_mode+1) for i_mode in range(InputData.n_modes)]+['C','D']\n",
    "\n",
    "FileName   = InputData.path_to_data_fld + '/test/ext/Input.csv'# + InputData.InputFile\n",
    "DataIn     = pd.read_csv(FileName, header=0)\n",
    "Input      = DataIn.to_numpy()\n",
    "VarNames   = list(DataIn.columns)\n",
    "\n",
    "FileName   = InputData.path_to_data_fld + '/test/ext/Output.csv'# + InputData.InputFile\n",
    "Data       = pd.read_csv(FileName, header=0)\n",
    "Output     = Data.to_numpy()\n",
    "\n",
    "FileName   = InputData.path_to_data_fld + '/train/pts/Input.csv'# + InputData.InputFile\n",
    "DataIn     = pd.read_csv(FileName, header=0)\n",
    "InputTrain = DataIn.to_numpy()\n",
    "VarNames   = list(DataIn.columns)\n",
    "\n",
    "FileName   = InputData.path_to_data_fld + '/train/pts/Output.csv'# + InputData.InputFile\n",
    "Data       = pd.read_csv(FileName, header=0)\n",
    "OutputTrain= Data.to_numpy()\n",
    "\n",
    "FileName   = InputData.path_to_data_fld + '/valid/pts/Input.csv'# + InputData.InputFile\n",
    "DataIn     = pd.read_csv(FileName, header=0)\n",
    "InputValid = DataIn.to_numpy()\n",
    "VarNames   = list(DataIn.columns)\n",
    "\n",
    "FileName   = InputData.path_to_data_fld + '/valid/pts/Output.csv'# + InputData.InputFile\n",
    "Data       = pd.read_csv(FileName, header=0)\n",
    "OutputValid= Data.to_numpy()\n",
    "\n",
    "## Variable to Be Visualized\n",
    "for i_mode in range(len(Vars)):\n",
    "    if (Vars[i_mode] == VarOI):\n",
    "        print('i_mode = ', i_mode)\n",
    "        break\n",
    "\n",
    "        \n",
    "yMat      = model.predict(Input)    \n",
    "yMatTrain = model.predict(InputTrain)    \n",
    "yMatValid = model.predict(InputValid)    \n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.plot(Input[:,iVar], Output[:,i_mode], 'ko')\n",
    "plt.plot(Input[:,iVar], yMat[:,i_mode], 'ro')\n",
    "# plt.plot(InputTrain[:,iPC], OutputTrain[:,iVar], 'ko')\n",
    "# plt.plot(InputTrain[:,iPC], yMatTrain[:,iVar], 'ro')\n",
    "# plt.plot(InputValid[:,iPC], OutputValid[:,iVar], 'bo')\n",
    "# plt.plot(InputValid[:,iPC], yMatValid[:,iVar], 'go')\n",
    "plt.xlabel(r'$\\eta_{'+str(iVar+1)+'}$')\n",
    "plt.ylabel(r'$'+VarOI+'$')\n",
    "#plt.xlim([1.e-6,1.e-2])\n",
    "#plt.legend()\n",
    "#plt.show()\n",
    "\n",
    "MinFact = 1.e-3\n",
    "MaxFact = 1.5\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.plot(OutputTrain[:,i_mode], yMatTrain[:,i_mode], 'ko')\n",
    "plt.plot(OutputValid[:,i_mode], yMatValid[:,i_mode], 'bo')\n",
    "plt.plot([yMatTrain[:,i_mode].min()-0.1, yMatTrain[:,i_mode].max()*MaxFact],[yMatTrain[:,i_mode].min()-0.1, yMatTrain[:,i_mode].max()*MaxFact], 'k-')\n",
    "plt.xlabel(r'$\\alpha_{x_'+str(i_mode+1)+'}$'+', from SVD')\n",
    "plt.ylabel(r'$\\alpha_{x_'+str(i_mode+1)+'}$'+', from FNN')\n",
    "#plt.xlim([yMatTrain[:,iVar].min()*MinFact, yMatTrain[:,iVar].max()*MaxFact])\n",
    "#plt.ylim([yMatTrain[:,iVar].min()*MinFact, yMatTrain[:,iVar].max()*MaxFact])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'a' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-cfb61e1da852>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# %matplotlib qt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0ma\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0miVar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'a' is not defined"
     ]
    }
   ],
   "source": [
    "# %matplotlib qt\n",
    "\n",
    "iVar = 3\n",
    "\n",
    "fig = plt.figure(figsize=(12,12))\n",
    "ax  = fig.add_subplot(projection='3d')\n",
    "ax.scatter(Input[:,0], Input[:,1], Output[:,iVar], c='k')\n",
    "ax.scatter(Input[:,0], Input[:,1], yMat[:,iVar], c='r')\n",
    "ax.set_xlabel(r'$\\eta_{'+str(1)+'}$')\n",
    "ax.set_ylabel(r'$\\eta_{'+str(2)+'}$')\n",
    "ax.set_zlabel(r'$'+Vars[iVar]+'$')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('[PCA]  Max % Error = ', np.max(abs((yMat - Output)/yMat)*100))\n",
    "# print('[PCA]  Max      SE = ', np.max((yMat - Output)**2))\n",
    "# print('[PCA] Mean % Error = ', np.mean(abs((yMat - Output)/yMat)*100))\n",
    "# print('[PCA]          MSE = ', np.mean((yMat - Output)**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'a' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-525d0a3b4312>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# OldRun  = 'Run_39'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# DataOld = pd.read_csv(path_to_run_fld+'/../'+OldRun+'/Training/History.csv')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0ma\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mfig\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mData\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'tot_loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m      \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Training'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'a' is not defined"
     ]
    }
   ],
   "source": [
    "Data    = pd.read_csv(path_to_run_fld+'/Training/History.csv')\n",
    "\n",
    "# OldRun  = 'Run_39'\n",
    "# DataOld = pd.read_csv(path_to_run_fld+'/../'+OldRun+'/Training/History.csv')\n",
    "\n",
    "fig  = plt.figure(figsize=(12,8))\n",
    "plt.plot(Data['tot_loss'],      label='Training')\n",
    "plt.plot(Data['val_tot_loss'],  label='Validation')\n",
    "# plt.plot(DataOld['tot_loss'],     label='Training, '+OldRun)\n",
    "# plt.plot(DataOld['val_tot_loss'], label='Validation, '+OldRun)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Tot. Loss [MSE]')\n",
    "plt.legend()\n",
    "plt.yscale('log')\n",
    "\n",
    "fig  = plt.figure(figsize=(12,8))\n",
    "plt.plot(Data['pts_loss'],      label='Training')\n",
    "plt.plot(Data['val_pts_loss'],  label='Validation')\n",
    "# plt.plot(DataOld['pts_loss'],     label='Training, '+OldRun)\n",
    "# plt.plot(DataOld['val_pts_loss'], label='Validation, '+OldRun)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Data Loss [MSE]')\n",
    "plt.legend()\n",
    "plt.yscale('log')\n",
    "#plt.ylim([1.e-2, 1.e0])\n",
    "\n",
    "# fig  = plt.figure(figsize=(12,8))\n",
    "# plt.plot(Data['scs_loss'],     label='Training')\n",
    "# plt.plot(Data['val_scs_loss'], label='Validation')\n",
    "# plt.xlabel('Epoch')\n",
    "# plt.ylabel('S.C.s Loss [MSE]')\n",
    "# plt.legend()\n",
    "# plt.yscale('log')\n",
    "\n",
    "# fig  = plt.figure(figsize=(12,8))\n",
    "# plt.plot(Data['ics_loss'],     label='Training')\n",
    "# plt.plot(Data['val_ics_loss'], label='Validation')\n",
    "# plt.xlabel('Epoch')\n",
    "# plt.ylabel('I.C.s Loss [MSE]')\n",
    "# plt.legend()\n",
    "# plt.yscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "\n",
    "# LName   = 'Trunk_1_HL' + str(len(InputData.ActFun[0]))\n",
    "\n",
    "# inputs  = tf.keras.Input(shape=(1,))\n",
    "\n",
    "# Output_ = inputs\n",
    "# for i in range(len(model.net.FNNLayersVecs[0])):\n",
    "#     Layer   = model.net.FNNLayersVecs[0][i]\n",
    "#     Output_ = Layer(Output_)\n",
    "\n",
    "\n",
    "# Branch = keras.Model(inputs=inputs, outputs=Output_)\n",
    "# Branch.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iVar = 1\n",
    "\n",
    "# SigmaVec = tf.math.softplus(0.05 * Branch.predict(Input)[:,64:])\n",
    "\n",
    "# fig = plt.figure()\n",
    "# ax  = fig.add_subplot(projection='3d')\n",
    "# ax.scatter(Input[:,0], Input[:,1], SigmaVec[:,iVar], c='k')\n",
    "# ax.set_xlabel(r'$\\eta_{'+str(1)+'}$')\n",
    "# ax.set_ylabel(r'$\\eta_{'+str(2)+'}$')\n",
    "# ax.set_zlabel(r'$\\Psi_{'+str(iVar+1)+'}$')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iVar = 63\n",
    "\n",
    "# MuVec = Branch.predict(Input)[:,0:64] * model.y_range + model.y_min\n",
    "\n",
    "# fig = plt.figure()\n",
    "# ax  = fig.add_subplot(projection='3d')\n",
    "# ax.scatter(Input[:,0], Input[:,1], Output[:,iVar], c='k')\n",
    "# ax.scatter(Input[:,0], Input[:,1], MuVec[:,iVar],    c='r')\n",
    "# ax.set_xlabel(r'$\\eta_{'+str(1)+'}$')\n",
    "# ax.set_ylabel(r'$\\eta_{'+str(2)+'}$')\n",
    "# ax.set_zlabel(r'$\\Psi_{'+str(iVar+1)+'}$')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import tensorflow as tf\n",
    "from keras    import backend\n",
    "from keras    import __version__ as keras_version  # pylint: disable=g-import-not-at-top\n",
    "from keras.saving import saving_utils\n",
    "\n",
    "\n",
    "def _legacy_weights(layer):\n",
    "    \"\"\"DO NOT USE.\n",
    "    For legacy reason, the layer.weights was in the order of\n",
    "    [self.trainable_weights + self.non_trainable_weights], and this order was\n",
    "    used for preserving the weights in h5 format. The new order of layer.weights\n",
    "    are the same as layer.get_weights() which is more intuitive for user. To\n",
    "    keep supporting the existing saved h5 file, this method should be used to\n",
    "    save/load weights. In future version, we will delete this method and\n",
    "    introduce a breaking change for h5 and stay with the new order for weights.\n",
    "    Args:\n",
    "    layer: a `tf.keras.Model` or `tf.keras.layers.Layer` instance.\n",
    "    Returns:\n",
    "    A list of variables with the order of trainable_weights, followed by\n",
    "      non_trainable_weights.\n",
    "    \"\"\"\n",
    "    weights = layer.trainable_weights + layer.non_trainable_weights\n",
    "    if any(not isinstance(w, tf.Variable) for w in weights):\n",
    "        raise NotImplementedError(\n",
    "            f'Save or restore weights that is not an instance of `tf.Variable` is '\n",
    "            f'not supported in h5, use `save_format=\\'tf\\'` instead. Received a '\n",
    "            f'model or layer {layer.__class__.__name__} with weights {weights}')\n",
    "    return weights\n",
    "\n",
    "\n",
    "\n",
    "def save_attributes_to_hdf5_group(group, name, data):\n",
    "    HDF5_OBJECT_HEADER_LIMIT = 64512\n",
    "    \n",
    "    bad_attributes = [x for x in data if len(x) > HDF5_OBJECT_HEADER_LIMIT]\n",
    "\n",
    "    # Expecting this to never be true.\n",
    "    if bad_attributes:\n",
    "        raise RuntimeError('The following attributes cannot be saved to HDF5 '\n",
    "                           'file because they are larger than %d bytes: %s' %\n",
    "                           (HDF5_OBJECT_HEADER_LIMIT, ', '.join(bad_attributes)))\n",
    "\n",
    "    data_npy = np.asarray(data)\n",
    "\n",
    "    num_chunks = 1\n",
    "    chunked_data = np.array_split(data_npy, num_chunks)\n",
    "\n",
    "    # This will never loop forever thanks to the test above.\n",
    "    while any(x.nbytes > HDF5_OBJECT_HEADER_LIMIT for x in chunked_data):\n",
    "        num_chunks += 1\n",
    "        chunked_data = np.array_split(data_npy, num_chunks)\n",
    "\n",
    "    if num_chunks > 1:\n",
    "        for chunk_id, chunk_data in enumerate(chunked_data):\n",
    "            group.attrs['%s%d' % (name, chunk_id)] = chunk_data\n",
    "    else:\n",
    "        group.attrs[name] = data\n",
    "\n",
    "        \n",
    "def load_attributes_from_hdf5_group(group, name):\n",
    "    \"\"\"Loads attributes of the specified name from the HDF5 group.\n",
    "    This method deals with an inherent problem\n",
    "    of HDF5 file which is not able to store\n",
    "    data larger than HDF5_OBJECT_HEADER_LIMIT bytes.\n",
    "    Args:\n",
    "      group: A pointer to a HDF5 group.\n",
    "      name: A name of the attributes to load.\n",
    "    Returns:\n",
    "      data: Attributes data.\n",
    "    \"\"\"\n",
    "    if name in group.attrs:\n",
    "        data = [\n",
    "            n.decode('utf8') if hasattr(n, 'decode') else n\n",
    "            for n in group.attrs[name]\n",
    "        ]\n",
    "    else:\n",
    "        data = []\n",
    "        chunk_id = 0\n",
    "        while '%s%d' % (name, chunk_id) in group.attrs:\n",
    "            data.extend([\n",
    "              n.decode('utf8') if hasattr(n, 'decode') else n\n",
    "              for n in group.attrs['%s%d' % (name, chunk_id)]\n",
    "          ])\n",
    "        chunk_id += 1\n",
    "    return data\n",
    "\n",
    "\n",
    "def save_weights_to_hdf5_group(f, layers, old_string, new_string):\n",
    "\n",
    "    layer_names_temp = load_attributes_from_hdf5_group(f_new, 'layer_names')\n",
    "    layer_names_old  = []\n",
    "    for layer in layer_names_temp:\n",
    "        layer_names_old.append(layer.encode('utf8'))\n",
    "    \n",
    "    layer_names = []\n",
    "    for layer in layers:\n",
    "        print(layer.name)\n",
    "        if (old_string in layer.name):\n",
    "            layer_name = layer.name.replace(old_string, new_string) #############\n",
    "            layer_names.append(layer_name.encode('utf8'))\n",
    "            layer._name = layer_name\n",
    "\n",
    "    save_attributes_to_hdf5_group(f, 'layer_names', layer_names+layer_names_old)\n",
    "    f.attrs['backend'] = backend.backend().encode('utf8')\n",
    "    f.attrs['keras_version'] = str(keras_version).encode('utf8')\n",
    "\n",
    "    # Sort model layers by layer name to ensure that group names are strictly\n",
    "    # growing to avoid prefix issues.\n",
    "    for layer in sorted(layers, key=lambda x: x.name):\n",
    "        if (new_string in layer.name):\n",
    "            layer_name    = layer.name.replace(old_string, new_string) ############# \n",
    "            g             = f.create_group(layer_name)\n",
    "            weights       = _legacy_weights(layer)\n",
    "            weight_values = backend.batch_get_value(weights)\n",
    "\n",
    "            weight_names = []\n",
    "            for w in weights:\n",
    "                w_name = w.name.replace(old_string, new_string) ############# \n",
    "                weight_names.append(w_name.encode('utf8'))\n",
    "            #weight_names = [w.name.encode('utf8') for w in weights]\n",
    "            save_attributes_to_hdf5_group(g, 'weight_names', weight_names)\n",
    "            for name, val in zip(weight_names, weight_values):\n",
    "                param_dset = g.create_dataset(name, val.shape, dtype=val.dtype)\n",
    "                if not val.shape:\n",
    "                    # scalar\n",
    "                    param_dset[()] = val\n",
    "                else:\n",
    "                    param_dset[:] = val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FNN-FNN_Normalization\n",
      "FNN-FNN-Main-HL_1\n",
      "dropout\n",
      "FNN-FNN-Main-HL_2\n",
      "dropout_1\n",
      "FNN-FNN-Main-HL_3\n",
      "dropout_2\n",
      "FNN-FNN-Main-HL_4\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    os.makedirs(InputData.path_to_data_fld+'/../../FNN/')\n",
    "except:\n",
    "    pass\n",
    "filename_new = InputData.path_to_data_fld+'/../../FNN/Final.h5'\n",
    "\n",
    "f_new        = h5py.File(filename_new, 'a')\n",
    "\n",
    "save_weights_to_hdf5_group(f_new, NN.layers, 'FNN-FNN', 'DeepONet-Branch_'+str(i_red))\n",
    "\n",
    "f_new.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_new.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_new        = h5py.File(filename_new, 'a')\n",
    "\n",
    "data = load_attributes_from_hdf5_group(f_new, 'layer_names')\n",
    "data\n",
    "\n",
    "f_new.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
