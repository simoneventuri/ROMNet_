{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Postprocessing a FNN Trained for a Mass-Spring-Damper System "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Loading Libraries\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import numpy                              as np\n",
    "import pandas                             as pd\n",
    "\n",
    "import matplotlib\n",
    "from matplotlib                           import pyplot as plt\n",
    "#WORKSPACE_PATH = os.environ['WORKSPACE_PATH']\n",
    "WORKSPACE_PATH = os.getcwd()+'/../../../../../../../'\n",
    "plt.style.use(WORKSPACE_PATH+'/ROMNet/romnet/extra/postprocessing/presentation.mplstyle')\n",
    "\n",
    "from tensorflow                           import keras\n",
    "\n",
    "import romnet                             as rmnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Input Data\n",
    "\n",
    "i_red          = 'All' # 1\n",
    "\n",
    "ROMNet_fld    = WORKSPACE_PATH + '/ROMNet/romnet/'\n",
    "#path_to_run_fld  = WORKSPACE_PATH + '/ROMNet/MSD_100Cases_POD_2_Trunk/FNN/Run_1/'\n",
    "path_to_run_fld  = WORKSPACE_PATH + '/ROMNet/MSD_100Cases_POD_All_Trunk/FNN/Run_1/'\n",
    "\n",
    "#path_to_run_fld  = WORKSPACE_PATH + '/ROMNet/MSD_100Cases_1_Trunk/FNN_BbB/Run_10/'\n",
    "#path_to_run_fld  = WORKSPACE_PATH + '/ROMNet/MSD_10Cases/FNN/BayesByBackprop/Run_2/'\n",
    "\n",
    "TrainingCases = [0]#[0,2,4,6,8]\n",
    "TestCases     = [0,2,4]#[0,2]\n",
    "\n",
    "NSamples      = 100\n",
    "\n",
    "Nt            = 100\n",
    "tout          = np.linspace(0.,15.,Nt)\n",
    "SOLVER        = 'Radau'\n",
    "\n",
    "LineTypeVec   = ['-',':','--','.-']*10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[ROMNet]: Reading Input File from:  /Users/sventuri/WORKSPACE/ROMNet/romnet/scripts/postprocessing/POD/MassSpringDamper/FNN/../../../../../../..//ROMNet/MSD_100Cases_POD_All_Trunk/FNN/Run_1/\n",
      "\n",
      "[ROMNet]: Keep Loading Modules and Functions...\n",
      "\n",
      "[ROMNet]: Initializing Input ...\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n[ROMNet]: Reading Input File from: \", path_to_run_fld)\n",
    "sys.path.insert(0, path_to_run_fld)\n",
    "\n",
    "print(\"\\n[ROMNet]: Keep Loading Modules and Functions...\")\n",
    "from ROMNet_Input import inputdata\n",
    "\n",
    "print(\"\\n[ROMNet]: Initializing Input ...\")\n",
    "InputData               = inputdata(WORKSPACE_PATH)\n",
    "\n",
    "\n",
    "InputData.InputFilePath = path_to_run_fld+'/ROMNet_Input.py'\n",
    "InputData.train_int_flg = 0\n",
    "InputData.path_to_run_fld  = path_to_run_fld"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[ROMNet - model_tf.py    ]:   Initializing the ML Model\n",
      "\n",
      "[ROMNet - model_tf.py    ]:   Building the ML Model\n",
      "\n",
      "[ROMNet - fnn.py                    ]:   Constructing Feed-Forward Network: \n",
      "[ROMNet - system_of_components.py   ]:     Constructing System of Components: FNN\n",
      "[ROMNet - component.py              ]:       Constructing Component: FNN\n",
      "[ROMNet - sub_component.py          ]:         Constructed Sub-Component: Main with Layers:       ListWrapper([<romnet.nn.building_blocks.normalization.CustomNormalization object at 0x16f4e8bb0>, <keras.layers.core.dense.Dense object at 0x16f745790>, <keras.layers.core.dropout.Dropout object at 0x16f923fd0>, <keras.layers.core.dense.Dense object at 0x16f92a190>, <keras.layers.core.dropout.Dropout object at 0x16f92ad00>, <keras.layers.core.dense.Dense object at 0x16f92acd0>, <keras.layers.core.dropout.Dropout object at 0x16f92a370>, <keras.layers.core.dense.Dense object at 0x16f8dbc70>])\n",
      "\n",
      "[ROMNet - model_tf.py    ]:   Loading ML Model Parameters from File:  /Users/sventuri/WORKSPACE/ROMNet/romnet/scripts/postprocessing/POD/MassSpringDamper/FNN/../../../../../../..//ROMNet/MSD_100Cases_POD_All_Trunk/FNN/Run_1//Training/Params//001346.h5\n",
      "[ROMNet - hdf5_format.py            ]:     Loaded Weights for Layer  FNN-FNN_Normalization\n",
      "[ROMNet - hdf5_format.py            ]:     Loaded Weights for Layer  FNN-FNN_Normalization\n",
      "[ROMNet - hdf5_format.py            ]:     Loaded Weights for Layer  FNN-FNN_Normalization\n",
      "[ROMNet - hdf5_format.py            ]:     Loaded Weights for Layer  FNN-FNN_Normalization\n",
      "[ROMNet - hdf5_format.py            ]:     Loaded Weights for Layer  FNN-FNN_Normalization\n",
      "[ROMNet - hdf5_format.py            ]:     Loaded Weights for Layer  FNN-FNN-Main-HL_1\n",
      "[ROMNet - hdf5_format.py            ]:     Loaded Weights for Layer  FNN-FNN-Main-HL_1\n",
      "[ROMNet - hdf5_format.py            ]:     Loaded Weights for Layer  FNN-FNN-Main-HL_2\n",
      "[ROMNet - hdf5_format.py            ]:     Loaded Weights for Layer  FNN-FNN-Main-HL_2\n",
      "[ROMNet - hdf5_format.py            ]:     Loaded Weights for Layer  FNN-FNN-Main-HL_3\n",
      "[ROMNet - hdf5_format.py            ]:     Loaded Weights for Layer  FNN-FNN-Main-HL_3\n",
      "[ROMNet - hdf5_format.py            ]:     Loaded Weights for Layer  FNN-FNN-Main-HL_4\n",
      "[ROMNet - hdf5_format.py            ]:     Loaded Weights for Layer  FNN-FNN-Main-HL_4\n",
      "'load_params' took 0.026435 s\n",
      "\n",
      "\n",
      "[ROMNet - model_tf.py    ]:   Saving ML Model Parameters to File:  /Users/sventuri/WORKSPACE/ROMNet/romnet/scripts/postprocessing/POD/MassSpringDamper/FNN/../../../../../../..//ROMNet/MSD_100Cases_POD_All_Trunk/FNN/Run_1//Model/Params/Initial.h5\n",
      "'save_params' took 0.015533 s\n",
      "\n",
      "'build' took 1.733147 s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "surrogate_type = InputData.surrogate_type\n",
    "if (surrogate_type == 'FNN-SourceTerms'):\n",
    "    surrogate_type = 'FNN'\n",
    "\n",
    "Net   = getattr(rmnt.nn, surrogate_type)\n",
    "\n",
    "model = rmnt.model.Model_TF(InputData)\n",
    "\n",
    "if (InputData.phys_system is not None):\n",
    "    System = getattr(rmnt.pinn.system, InputData.phys_system)\n",
    "    system = System(InputData)\n",
    "    \n",
    "model.build(InputData, None, Net, system)#, loadfile_no='000027')\n",
    "\n",
    "NN    = model.net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'a' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-ecd78b3213a5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m### Evaluating on Test Data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0ma\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mVarOI\u001b[0m     \u001b[0;34m=\u001b[0m \u001b[0;34m'POD_1'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mVars\u001b[0m      \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'POD_'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi_mode\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi_mode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mInputData\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_modes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'a' is not defined"
     ]
    }
   ],
   "source": [
    "### Evaluating on Test Data\n",
    "\n",
    "VarOI     = 'POD_1'\n",
    "\n",
    "Vars      = ['POD_'+str(i_mode+1) for i_mode in range(InputData.n_modes)]\n",
    "\n",
    "FileName  = InputData.path_to_data_fld + '/test/ext/Input.csv'# + InputData.InputFile\n",
    "Data      = pd.read_csv(FileName, header=0)\n",
    "tVec      = Data['t'].to_numpy()[...,np.newaxis]\n",
    "VarNames  = list(Data.columns).remove('t')\n",
    "\n",
    "FileName  = InputData.path_to_data_fld + '/test/ext/Output.csv'# + InputData.InputFile\n",
    "Data      = pd.read_csv(FileName, header=0)\n",
    "Output    = Data[Vars].to_numpy()\n",
    "\n",
    "\n",
    "## Variable to Be Visualized\n",
    "for iVar in range(len(Vars)):\n",
    "    if (Vars[iVar] == VarOI):\n",
    "        print('iVar = ', iVar)\n",
    "        break\n",
    "\n",
    "        \n",
    "yMat = NN.predict(tVec)    \n",
    "\n",
    "plt.figure(figsize=(10,8))\n",
    "plt.plot(tVec, Output[:,iVar],  'k', label='From SVD')\n",
    "plt.plot(tVec, yMat[:,iVar],    'r', label='From FNN')\n",
    "plt.xlabel('t')\n",
    "plt.ylabel(r'$\\Psi_{x_'+str(iVar+1)+'}$')\n",
    "#plt.xscale('log')\n",
    "#plt.xlim([1.e-6,1.e-2])\n",
    "plt.legend()\n",
    "#plt.show()\n",
    "\n",
    "\n",
    "tVecTtry = np.linspace(0., 30., 3000)[...,np.newaxis]\n",
    "yMatTry  = NN.predict(tVecTtry)    \n",
    "\n",
    "plt.figure(figsize=(10,8))\n",
    "plt.plot(tVec, Output[:,iVar],      'k', label='From SVD')\n",
    "plt.plot(tVecTtry, yMatTry[:,iVar], 'b', label='From FNN')\n",
    "plt.xlabel('t')\n",
    "plt.ylabel(r'$\\Psi_{x_'+str(iVar+1)+'}$')\n",
    "#plt.xscale('log')\n",
    "#plt.xlim([1.e-6,1.e-2])\n",
    "plt.legend()\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PCA]  Max % Error =  513983.28007721284\n",
      "[PCA]  Max      SE =  45.43843942279441\n",
      "[PCA] Mean % Error =  944.6142423725013\n",
      "[PCA]          MSE =  1.0010657537061833\n"
     ]
    }
   ],
   "source": [
    "print('[PCA]  Max % Error = ', np.max(abs((yMat - Output)/yMat)*100))\n",
    "print('[PCA]  Max      SE = ', np.max((yMat - Output)**2))\n",
    "print('[PCA] Mean % Error = ', np.mean(abs((yMat - Output)/yMat)*100))\n",
    "print('[PCA]          MSE = ', np.mean((yMat - Output)**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Users/sventuri/WORKSPACE/ROMNet/romnet/scripts/postprocessing/POD/MassSpringDamper/FNN/../../../../../../..//ROMNet/MSD_100Cases_POD_All_Trunk/FNN/Run_1//Training/History.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-0698eb42463d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mData\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_to_run_fld\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'/Training/History.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mfig\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mData\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'tot_loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m     \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Training'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mData\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_tot_loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Validation'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    684\u001b[0m     )\n\u001b[1;32m    685\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 686\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    688\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    450\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 452\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    453\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    454\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    934\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    935\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 936\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    937\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    938\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1166\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1167\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1168\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1169\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1170\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1996\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1997\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1998\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1999\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2000\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Users/sventuri/WORKSPACE/ROMNet/romnet/scripts/postprocessing/POD/MassSpringDamper/FNN/../../../../../../..//ROMNet/MSD_100Cases_POD_All_Trunk/FNN/Run_1//Training/History.csv'"
     ]
    }
   ],
   "source": [
    "Data = pd.read_csv(path_to_run_fld+'/Training/History.csv')\n",
    "\n",
    "fig  = plt.figure(figsize=(12,8))\n",
    "plt.plot(Data['tot_loss'],     label='Training')\n",
    "plt.plot(Data['val_tot_loss'], label='Validation')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Tot. Loss [MSE]')\n",
    "plt.legend()\n",
    "plt.yscale('log')\n",
    "\n",
    "fig  = plt.figure(figsize=(12,8))\n",
    "plt.plot(Data['pts_loss'],     label='Training')\n",
    "plt.plot(Data['val_pts_loss'], label='Validation')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Data Loss [MSE]')\n",
    "plt.legend()\n",
    "plt.yscale('log')\n",
    "#plt.ylim([1.e-2, 1.e0])\n",
    "# fig  = plt.figure(figsize=(12,8))\n",
    "# plt.plot(Data['scs_loss'],     label='Training')\n",
    "# plt.plot(Data['val_scs_loss'], label='Validation')\n",
    "# plt.xlabel('Epoch')\n",
    "# plt.ylabel('S.C.s Loss [MSE]')\n",
    "# plt.legend()\n",
    "# plt.yscale('log')\n",
    "\n",
    "# fig  = plt.figure(figsize=(12,8))\n",
    "# plt.plot(Data['ics_loss'],     label='Training')\n",
    "# plt.plot(Data['val_ics_loss'], label='Validation')\n",
    "# plt.xlabel('Epoch')\n",
    "# plt.ylabel('I.C.s Loss [MSE]')\n",
    "# plt.legend()\n",
    "# plt.yscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "OutputDir = InputData.path_to_data_fld\n",
    "\n",
    "FileName = OutputDir+'/../ROM/A.csv'\n",
    "A        = pd.read_csv(FileName, delimiter=',', header=None).to_numpy()\n",
    "\n",
    "FileName = OutputDir+'/../ROM/C.csv'\n",
    "C        = pd.read_csv(FileName, delimiter=',', header=None).to_numpy()\n",
    "\n",
    "FileName = OutputDir+'/../ROM/D.csv'\n",
    "D        = pd.read_csv(FileName, delimiter=',', header=None).to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'a' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-f96dbd7851c7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0myMat_pod_\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mNN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtVec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0myMat_\u001b[0m      \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0myMat_pod_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mD\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0ma\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtVec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myMat_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;34m'k'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'a' is not defined"
     ]
    }
   ],
   "source": [
    "yMat_pod_  = NN.predict(tVec)    \n",
    "yMat_      = (yMat_pod_.dot(A))*D[:,0] + C[:,0]\n",
    "\n",
    "plt.figure(figsize=(12,10))\n",
    "plt.plot(tVec, yMat_[:,50],  'k')\n",
    "plt.xlabel('t')\n",
    "#plt.xscale('log')\n",
    "#plt.legend()\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'a' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-52293c023342>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0ma\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'a' is not defined"
     ]
    }
   ],
   "source": [
    "plt.plot(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import tensorflow as tf\n",
    "from keras    import backend\n",
    "from keras    import __version__ as keras_version  # pylint: disable=g-import-not-at-top\n",
    "from keras.saving import saving_utils\n",
    "\n",
    "\n",
    "def _legacy_weights(layer):\n",
    "    \"\"\"DO NOT USE.\n",
    "    For legacy reason, the layer.weights was in the order of\n",
    "    [self.trainable_weights + self.non_trainable_weights], and this order was\n",
    "    used for preserving the weights in h5 format. The new order of layer.weights\n",
    "    are the same as layer.get_weights() which is more intuitive for user. To\n",
    "    keep supporting the existing saved h5 file, this method should be used to\n",
    "    save/load weights. In future version, we will delete this method and\n",
    "    introduce a breaking change for h5 and stay with the new order for weights.\n",
    "    Args:\n",
    "    layer: a `tf.keras.Model` or `tf.keras.layers.Layer` instance.\n",
    "    Returns:\n",
    "    A list of variables with the order of trainable_weights, followed by\n",
    "      non_trainable_weights.\n",
    "    \"\"\"\n",
    "    weights = layer.trainable_weights + layer.non_trainable_weights\n",
    "    if any(not isinstance(w, tf.Variable) for w in weights):\n",
    "        raise NotImplementedError(\n",
    "            f'Save or restore weights that is not an instance of `tf.Variable` is '\n",
    "            f'not supported in h5, use `save_format=\\'tf\\'` instead. Received a '\n",
    "            f'model or layer {layer.__class__.__name__} with weights {weights}')\n",
    "    return weights\n",
    "\n",
    "\n",
    "\n",
    "def save_attributes_to_hdf5_group(group, name, data):\n",
    "    HDF5_OBJECT_HEADER_LIMIT = 64512\n",
    "    \n",
    "    bad_attributes = [x for x in data if len(x) > HDF5_OBJECT_HEADER_LIMIT]\n",
    "\n",
    "    # Expecting this to never be true.\n",
    "    if bad_attributes:\n",
    "        raise RuntimeError('The following attributes cannot be saved to HDF5 '\n",
    "                           'file because they are larger than %d bytes: %s' %\n",
    "                           (HDF5_OBJECT_HEADER_LIMIT, ', '.join(bad_attributes)))\n",
    "\n",
    "    data_npy = np.asarray(data)\n",
    "\n",
    "    num_chunks = 1\n",
    "    chunked_data = np.array_split(data_npy, num_chunks)\n",
    "\n",
    "    # This will never loop forever thanks to the test above.\n",
    "    while any(x.nbytes > HDF5_OBJECT_HEADER_LIMIT for x in chunked_data):\n",
    "        num_chunks += 1\n",
    "        chunked_data = np.array_split(data_npy, num_chunks)\n",
    "\n",
    "    if num_chunks > 1:\n",
    "        for chunk_id, chunk_data in enumerate(chunked_data):\n",
    "            group.attrs['%s%d' % (name, chunk_id)] = chunk_data\n",
    "    else:\n",
    "        group.attrs[name] = data\n",
    "\n",
    "        \n",
    "def load_attributes_from_hdf5_group(group, name):\n",
    "    \"\"\"Loads attributes of the specified name from the HDF5 group.\n",
    "    This method deals with an inherent problem\n",
    "    of HDF5 file which is not able to store\n",
    "    data larger than HDF5_OBJECT_HEADER_LIMIT bytes.\n",
    "    Args:\n",
    "      group: A pointer to a HDF5 group.\n",
    "      name: A name of the attributes to load.\n",
    "    Returns:\n",
    "      data: Attributes data.\n",
    "    \"\"\"\n",
    "    if name in group.attrs:\n",
    "        data = [\n",
    "            n.decode('utf8') if hasattr(n, 'decode') else n\n",
    "            for n in group.attrs[name]\n",
    "        ]\n",
    "    else:\n",
    "        data = []\n",
    "        chunk_id = 0\n",
    "        while '%s%d' % (name, chunk_id) in group.attrs:\n",
    "            data.extend([\n",
    "              n.decode('utf8') if hasattr(n, 'decode') else n\n",
    "              for n in group.attrs['%s%d' % (name, chunk_id)]\n",
    "          ])\n",
    "        chunk_id += 1\n",
    "    return data\n",
    "\n",
    "\n",
    "def save_weights_to_hdf5_group(f, layers, old_string, new_string):\n",
    "\n",
    "    layer_names_temp = load_attributes_from_hdf5_group(f_new, 'layer_names')\n",
    "    layer_names_old  = []\n",
    "    for layer in layer_names_temp:\n",
    "        layer_names_old.append(layer.encode('utf8'))\n",
    "    \n",
    "    layer_names = []\n",
    "    for layer in layers:\n",
    "        if (old_string in layer.name):\n",
    "            layer_name = layer.name.replace(old_string, new_string) #############\n",
    "            layer_names.append(layer_name.encode('utf8'))\n",
    "            layer._name = layer_name\n",
    "\n",
    "    save_attributes_to_hdf5_group(f, 'layer_names', layer_names+layer_names_old)\n",
    "    f.attrs['backend'] = backend.backend().encode('utf8')\n",
    "    f.attrs['keras_version'] = str(keras_version).encode('utf8')\n",
    "\n",
    "    # Sort model layers by layer name to ensure that group names are strictly\n",
    "    # growing to avoid prefix issues.\n",
    "    for layer in sorted(layers, key=lambda x: x.name):\n",
    "        if (new_string in layer.name):\n",
    "            layer_name    = layer.name.replace(old_string, new_string) ############# \n",
    "            g             = f.create_group(layer_name)\n",
    "            weights       = _legacy_weights(layer)\n",
    "            weight_values = backend.batch_get_value(weights)\n",
    "\n",
    "            weight_names = []\n",
    "            for w in weights:\n",
    "                w_name = w.name.replace(old_string, new_string) ############# \n",
    "                weight_names.append(w_name.encode('utf8'))\n",
    "            #weight_names = [w.name.encode('utf8') for w in weights]\n",
    "            save_attributes_to_hdf5_group(g, 'weight_names', weight_names)\n",
    "            for name, val in zip(weight_names, weight_values):\n",
    "                param_dset = g.create_dataset(name, val.shape, dtype=val.dtype)\n",
    "                if not val.shape:\n",
    "                    # scalar\n",
    "                    param_dset[()] = val\n",
    "                else:\n",
    "                    param_dset[:] = val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    os.makedirs(InputData.path_to_data_fld+'/../../FNN/')\n",
    "except:\n",
    "    pass\n",
    "filename_new = InputData.path_to_data_fld+'/../../FNN/Final.h5'\n",
    "\n",
    "f_new        = h5py.File(filename_new, 'a')\n",
    "\n",
    "if (i_red == 'All'):\n",
    "    save_weights_to_hdf5_group(f_new, NN.layers, 'FNN-FNN', 'DeepONet-Trunk')\n",
    "else:\n",
    "    save_weights_to_hdf5_group(f_new, NN.layers, 'FNN-FNN', 'DeepONet-Trunk_'+str(i_red))\n",
    "    \n",
    "f_new.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f_new        = h5py.File(filename_new, 'a')\n",
    "\n",
    "# data = load_attributes_from_hdf5_group(f_new, 'layer_names')\n",
    "# data\n",
    "\n",
    "# f_new.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_new.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
